{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":742210,"sourceType":"datasetVersion","datasetId":17}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch.optim as optim\nimport torchdata.datapipes as dp\nimport torchtext.transforms as T\nimport spacy\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm, trange\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-05T14:46:59.436681Z","iopub.execute_input":"2024-04-05T14:46:59.437350Z","iopub.status.idle":"2024-04-05T14:46:59.450810Z","shell.execute_reply.started":"2024-04-05T14:46:59.437303Z","shell.execute_reply":"2024-04-05T14:46:59.449512Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntweets = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')\ntexts = tweets['text']\nlabels = tweets['airline_sentiment']\ntweets = pd.DataFrame(list(zip(texts, labels)), columns=['texts', 'labels'])\ntweets","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:47:03.502620Z","iopub.execute_input":"2024-04-05T14:47:03.503467Z","iopub.status.idle":"2024-04-05T14:47:03.664477Z","shell.execute_reply.started":"2024-04-05T14:47:03.503413Z","shell.execute_reply":"2024-04-05T14:47:03.662655Z"},"trusted":true},"execution_count":112,"outputs":[{"execution_count":112,"output_type":"execute_result","data":{"text/plain":"                                                   texts    labels\n0                    @VirginAmerica What @dhepburn said.   neutral\n1      @VirginAmerica plus you've added commercials t...  positive\n2      @VirginAmerica I didn't today... Must mean I n...   neutral\n3      @VirginAmerica it's really aggressive to blast...  negative\n4      @VirginAmerica and it's a really big bad thing...  negative\n...                                                  ...       ...\n14635  @AmericanAir thank you we got on a different f...  positive\n14636  @AmericanAir leaving over 20 minutes Late Flig...  negative\n14637  @AmericanAir Please bring American Airlines to...   neutral\n14638  @AmericanAir you have my money, you change my ...  negative\n14639  @AmericanAir we have 8 ppl so we need 2 know h...   neutral\n\n[14640 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>texts</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@VirginAmerica What @dhepburn said.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@VirginAmerica plus you've added commercials t...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@VirginAmerica it's really aggressive to blast...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@VirginAmerica and it's a really big bad thing...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14635</th>\n      <td>@AmericanAir thank you we got on a different f...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>14636</th>\n      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>14637</th>\n      <td>@AmericanAir Please bring American Airlines to...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>14638</th>\n      <td>@AmericanAir you have my money, you change my ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>14639</th>\n      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n<p>14640 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"t = []  # Initialize an empty list to store all words\nfor text in tweets['texts']:\n    text = text.lower().replace('@','').split()\n    t.extend(text)  # Extend the list with individual words instead of appending the whole split text\n\nt = set(t)  # Convert the list to a set to get unique words\nword_to_idx = {word: i for i, word in enumerate(t)}  # Create word-to-index mapping\nprint(\"Length of vocabulary:\", len(word_to_idx))","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:47:26.686837Z","iopub.execute_input":"2024-04-05T14:47:26.687726Z","iopub.status.idle":"2024-04-05T14:47:26.807409Z","shell.execute_reply.started":"2024-04-05T14:47:26.687677Z","shell.execute_reply":"2024-04-05T14:47:26.805916Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"Length of vocabulary: 26764\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess(df):\n    df['texts'] = df['texts'].apply(lambda x: x.lower().replace('@',''))\n    df['tokenized_texts'] = df['texts'].apply(lambda x: x.split())  # Tokenize the sentence\n    return df\n\ntweets = preprocess(tweets)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:47:29.720139Z","iopub.execute_input":"2024-04-05T14:47:29.720652Z","iopub.status.idle":"2024-04-05T14:47:29.795895Z","shell.execute_reply.started":"2024-04-05T14:47:29.720618Z","shell.execute_reply":"2024-04-05T14:47:29.794602Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"# Train Word2Vec model\nmodel = Word2Vec(sentences=tweets['tokenized_texts'], vector_size=100, window=5, min_count=1, workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:54:42.864156Z","iopub.execute_input":"2024-04-05T14:54:42.864691Z","iopub.status.idle":"2024-04-05T14:54:45.420637Z","shell.execute_reply.started":"2024-04-05T14:54:42.864656Z","shell.execute_reply":"2024-04-05T14:54:45.418405Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"# Generate embeddings for each tokenized text\nembeddings = [model.wv[tokenized_text] for tokenized_text in tweets['tokenized_texts']]","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:54:56.128615Z","iopub.execute_input":"2024-04-05T14:54:56.129126Z","iopub.status.idle":"2024-04-05T14:54:57.280114Z","shell.execute_reply.started":"2024-04-05T14:54:56.129092Z","shell.execute_reply":"2024-04-05T14:54:57.278109Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"# Define a custom dataset class\nclass TextDataset(Dataset):\n    def __init__(self, embeddings, labels):\n        self.embeddings = embeddings\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.embeddings)\n    \n    def __getitem__(self, idx):\n        return torch.tensor(self.embeddings[idx]), torch.tensor(self.labels[idx])\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Encode labels\nlabels_encoded = label_encoder.fit_transform(labels)\n\n# Split data into training and validation sets\n\ntrain_embeddings, val_embeddings, train_labels, val_labels = train_test_split(embeddings, labels_encoded, test_size=0.2, random_state=42)\n\n# Create datasets and dataloaders\ntrain_dataset = TextDataset(train_embeddings, train_labels)\nval_dataset = TextDataset(val_embeddings, val_labels)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: (pad_sequence([item[0] for item in batch]), torch.tensor([item[1] for item in batch])))\nval_loader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:00:22.455581Z","iopub.execute_input":"2024-04-05T15:00:22.456311Z","iopub.status.idle":"2024-04-05T15:00:22.484651Z","shell.execute_reply.started":"2024-04-05T15:00:22.456134Z","shell.execute_reply":"2024-04-05T15:00:22.483669Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"# Define the Generator and Discriminator classes\nclass Generator(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, output_size)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_size):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_size, 128),  # Adjust input_size to match the output size of the Generator\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:01:19.029575Z","iopub.execute_input":"2024-04-05T15:01:19.030060Z","iopub.status.idle":"2024-04-05T15:01:19.041761Z","shell.execute_reply.started":"2024-04-05T15:01:19.030025Z","shell.execute_reply":"2024-04-05T15:01:19.039585Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"# Define the size of the embeddings\nembed_size = 100\n\n# Define the size of the noise vector\nnoise_size = 100\n\n# Instantiate Generator and Discriminator\ngenerator = Generator(input_size=noise_size, hidden_size=128, output_size=embed_size)\ndiscriminator = Discriminator(input_size=embed_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:06:14.030001Z","iopub.execute_input":"2024-04-05T15:06:14.031382Z","iopub.status.idle":"2024-04-05T15:06:14.042010Z","shell.execute_reply.started":"2024-04-05T15:06:14.031340Z","shell.execute_reply":"2024-04-05T15:06:14.040403Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"# Define loss function and optimizers\ncriterion = nn.BCELoss()\noptimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.001)\noptimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:01:27.708498Z","iopub.execute_input":"2024-04-05T15:01:27.709021Z","iopub.status.idle":"2024-04-05T15:01:27.718139Z","shell.execute_reply.started":"2024-04-05T15:01:27.708984Z","shell.execute_reply":"2024-04-05T15:01:27.716441Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"# Training loop with tqdm\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    loop = tqdm(train_loader, leave=True)\n    for real_data, _ in loop:\n        batch_size = real_data.size(0)\n        real_data = real_data.float()  # Convert to float\n        real_data = real_data.to(device)\n\n        # Train Discriminator\n        optimizer_disc.zero_grad()\n        disc_real = discriminator(real_data)\n        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))  # Real data should be classified as 1\n        loss_disc_real.backward()\n\n        noise = torch.rand(batch_size, noise_size).to(device)\n        fake_data = generator(noise)\n        disc_fake = discriminator(fake_data.detach())\n        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))  # Fake data should be classified as 0\n        loss_disc_fake.backward()\n        \n        optimizer_disc.step()\n\n        # Train Generator\n        optimizer_gen.zero_grad()\n        disc_fake = discriminator(fake_data)\n        loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))  # Generator wants to fool the discriminator, so labels are 1\n        loss_gen.backward()\n        optimizer_gen.step()\n\n        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        loop.set_postfix(gen_loss=loss_gen.item(), disc_loss=loss_disc_real.item() + loss_disc_fake.item())\n\n    if (epoch + 1) % 10 == 0:\n        print(f'\\nEpoch {epoch+1}/{num_epochs}:\\t Generator loss: {loss_gen.item():.4f}\\t Discriminator loss: {loss_disc_real.item() + loss_disc_fake.item():.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:07:47.986443Z","iopub.execute_input":"2024-04-05T15:07:47.987192Z","iopub.status.idle":"2024-04-05T15:11:33.390598Z","shell.execute_reply.started":"2024-04-05T15:07:47.987126Z","shell.execute_reply":"2024-04-05T15:11:33.389006Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stderr","text":"Epoch [1/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 74.22it/s, disc_loss=1.46, gen_loss=0.75] \nEpoch [2/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 81.72it/s, disc_loss=1.46, gen_loss=0.752]\nEpoch [3/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:05<00:00, 68.11it/s, disc_loss=1.46, gen_loss=0.75] \nEpoch [4/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 73.95it/s, disc_loss=1.45, gen_loss=0.753]\nEpoch [5/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 79.01it/s, disc_loss=1.45, gen_loss=0.752]\nEpoch [6/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 79.98it/s, disc_loss=1.45, gen_loss=0.75] \nEpoch [7/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 90.79it/s, disc_loss=1.46, gen_loss=0.75]  \nEpoch [8/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 77.11it/s, disc_loss=1.46, gen_loss=0.751]\nEpoch [9/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 77.04it/s, disc_loss=1.46, gen_loss=0.753]\nEpoch [10/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 74.62it/s, disc_loss=1.44, gen_loss=0.753]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/50:\t Generator loss: 0.7526\t Discriminator loss: 1.4412\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 77.84it/s, disc_loss=1.45, gen_loss=0.75] \nEpoch [12/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 81.23it/s, disc_loss=1.45, gen_loss=0.749]\nEpoch [13/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 77.77it/s, disc_loss=1.46, gen_loss=0.75] \nEpoch [14/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:05<00:00, 73.19it/s, disc_loss=1.45, gen_loss=0.75] \nEpoch [15/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 86.26it/s, disc_loss=1.46, gen_loss=0.751]\nEpoch [16/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 76.92it/s, disc_loss=1.47, gen_loss=0.749]\nEpoch [17/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 75.80it/s, disc_loss=1.45, gen_loss=0.752]\nEpoch [18/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 81.44it/s, disc_loss=1.45, gen_loss=0.754]\nEpoch [19/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 83.94it/s, disc_loss=1.46, gen_loss=0.748] \nEpoch [20/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 76.94it/s, disc_loss=1.45, gen_loss=0.756]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20/50:\t Generator loss: 0.7561\t Discriminator loss: 1.4529\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 78.55it/s, disc_loss=1.45, gen_loss=0.753]\nEpoch [22/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 82.11it/s, disc_loss=1.46, gen_loss=0.752]\nEpoch [23/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 88.79it/s, disc_loss=1.45, gen_loss=0.751] \nEpoch [24/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 88.16it/s, disc_loss=1.45, gen_loss=0.752] \nEpoch [25/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 80.97it/s, disc_loss=1.45, gen_loss=0.751]\nEpoch [26/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 77.43it/s, disc_loss=1.45, gen_loss=0.753]\nEpoch [27/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 77.49it/s, disc_loss=1.45, gen_loss=0.755]\nEpoch [28/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 85.87it/s, disc_loss=1.46, gen_loss=0.751]\nEpoch [29/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 75.73it/s, disc_loss=1.45, gen_loss=0.752]\nEpoch [30/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 80.85it/s, disc_loss=1.45, gen_loss=0.752] \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 30/50:\t Generator loss: 0.7520\t Discriminator loss: 1.4453\n","output_type":"stream"},{"name":"stderr","text":"Epoch [31/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 86.19it/s, disc_loss=1.46, gen_loss=0.748]\nEpoch [32/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 77.41it/s, disc_loss=1.45, gen_loss=0.751]\nEpoch [33/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 81.81it/s, disc_loss=1.46, gen_loss=0.748]\nEpoch [34/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 85.19it/s, disc_loss=1.46, gen_loss=0.749]\nEpoch [35/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 89.34it/s, disc_loss=1.45, gen_loss=0.747]\nEpoch [36/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:03<00:00, 99.50it/s, disc_loss=1.46, gen_loss=0.75]  \nEpoch [37/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 85.06it/s, disc_loss=1.45, gen_loss=0.75]  \nEpoch [38/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 83.86it/s, disc_loss=1.45, gen_loss=0.753]\nEpoch [39/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 89.00it/s, disc_loss=1.44, gen_loss=0.752] \nEpoch [40/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 79.95it/s, disc_loss=1.47, gen_loss=0.75] \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 40/50:\t Generator loss: 0.7498\t Discriminator loss: 1.4660\n","output_type":"stream"},{"name":"stderr","text":"Epoch [41/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:03<00:00, 93.50it/s, disc_loss=1.45, gen_loss=0.752] \nEpoch [42/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 78.61it/s, disc_loss=1.46, gen_loss=0.751]\nEpoch [43/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 79.31it/s, disc_loss=1.46, gen_loss=0.751]\nEpoch [44/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 79.23it/s, disc_loss=1.45, gen_loss=0.754] \nEpoch [45/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 85.30it/s, disc_loss=1.44, gen_loss=0.754] \nEpoch [46/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 88.01it/s, disc_loss=1.45, gen_loss=0.75] \nEpoch [47/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:05<00:00, 70.73it/s, disc_loss=1.47, gen_loss=0.751]\nEpoch [48/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 84.09it/s, disc_loss=1.45, gen_loss=0.751]\nEpoch [49/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:03<00:00, 97.92it/s, disc_loss=1.45, gen_loss=0.754] \nEpoch [50/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:04<00:00, 90.67it/s, disc_loss=1.46, gen_loss=0.751] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 50/50:\t Generator loss: 0.7511\t Discriminator loss: 1.4592\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate noise vector\nnoise_vector = torch.rand(1, noise_size).to(device)\n\n# Pass noise vector through generator\ngenerated_embeddings = generator(noise_vector)\n\n# Convert embeddings to text\n# For example, if you have a Word2Vec model, you can find the closest words to the generated embeddings\n# and construct a sentence from those words\ngenerated_text = []\nfor embedding in generated_embeddings:\n    # Find closest word to each embedding\n    closest_word = model.wv.similar_by_vector(embedding.cpu().detach().numpy(), topn=1)[0][0]\n    generated_text.append(closest_word)\n\n# Join words to form a sentence\ngenerated_sentence = ' '.join(generated_text)\n\nprint(\"Generated Sentence:\", generated_sentence)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:15:28.179553Z","iopub.execute_input":"2024-04-05T15:15:28.180063Z","iopub.status.idle":"2024-04-05T15:15:28.215386Z","shell.execute_reply.started":"2024-04-05T15:15:28.180021Z","shell.execute_reply":"2024-04-05T15:15:28.213454Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"Generated Sentence: #whyabcwhy\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate 10 sentences\nnum_sentences = 10\ngenerated_sentences = []\n\nfor _ in range(num_sentences):\n    # Generate noise vector\n    noise_vector = torch.rand(1, noise_size).to(device)\n\n    # Pass noise vector through generator\n    generated_embeddings = generator(noise_vector)\n\n    # Convert embeddings to text\n    generated_text = []\n    for embedding in generated_embeddings:\n        # Find closest word to each embedding\n        closest_word = model.wv.similar_by_vector(embedding.cpu().detach().numpy(), topn=1)[0][0]\n        generated_text.append(closest_word)\n\n    # Join words to form a sentence\n    generated_sentence = ' '.join(generated_text)\n    \n    # Append generated sentence to the list\n    generated_sentences.append(generated_sentence)\n\n# Print generated sentences\nfor i, sentence in enumerate(generated_sentences, 1):\n    print(f\"Sentence {i}: {sentence}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:16:51.626652Z","iopub.execute_input":"2024-04-05T15:16:51.627201Z","iopub.status.idle":"2024-04-05T15:16:51.792148Z","shell.execute_reply.started":"2024-04-05T15:16:51.627143Z","shell.execute_reply":"2024-04-05T15:16:51.790947Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"Sentence 1: ground,\nSentence 2: terminal?\nSentence 3: terminal?\nSentence 4: terminal?\nSentence 5: #unitedhatesusall\nSentence 6: #unitedhatesusall\nSentence 7: ground,\nSentence 8: terminal?\nSentence 9: http://t.co/7z3gqebfk2\nSentence 10: 695\n","output_type":"stream"}]}]}