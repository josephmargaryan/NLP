## NLP Methods Implemented in the Repository

### Spam Email Classification

For the task of classifying spam emails, we employ various vectorization techniques to transform the email content into numerical features suitable for supervised classification models. Specifically, we utilize:

- **Count Vectorizer**: This technique creates a vector representation of each email by counting the occurrences of each token. It effectively captures the frequency of different words in the document.
  
- **Term Frequency-Inverse Document Frequency (TF-IDF)**: TF-IDF computes a statistical measure to represent the importance of each word in the document relative to the entire corpus. It creates a sparse matrix with frequency values of each token based on the entire document. This method is particularly useful for capturing the significance of words within specific documents.

For classification, we employ popular machine learning algorithms including:

- **Multinomial Naive Bayes**: A probabilistic model that assumes independence between features. It is commonly used for text classification tasks.
  
- **Logistic Regression**: A linear model that predicts the probability of a binary outcome. It's a simple yet effective algorithm for classification tasks.
  
- **Support Vector Machine (SVM)**: SVM applies decision boundaries to separate data points into different classes. It aims to find the optimal hyperplane that maximizes the margin between classes.

### Transformer-Based Sentiment Analysis

In addition to traditional methods, we implement a transformer-based model for sentiment analysis. The transformer architecture comprises the following components:

- **Embedding Layer**: With 50 trainable weights, the embedding layer captures the semantic meaning of each token in the text. It updates the weights through gradient descent optimization using the Adam optimizer and cross-entropy loss function.
  
- **Positional Embedding Layer**: Unlike the trainable embedding layer, the positional embedding layer utilizes fixed values based on sine and cosine functions to encode positional information of tokens. It enhances the model's understanding of token positions relative to each other.
  
- **Multihead Attention Mechanism**: The central component of the transformer, the multihead attention mechanism, facilitates capturing relationships between tokens. It operates permutation-invariantly, meaning the model remains unaffected even if the input token order changes. The attention mechanism computes a weighted sum of values (`V`) based on the attention scores calculated from query (`Q`) and key (`K`) matrices.

  ```math
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V

<img width="414" alt="SkÃ¦rmbillede 2024-04-05 kl  19 34 40" src="https://github.com/josephmargaryan/NLP/assets/126695370/ccca2673-cf28-4172-b9ef-6a244af30948">

